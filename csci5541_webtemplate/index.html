<!DOCTYPE html>
<html lang="en-US">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project | Spring 2025 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />
  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">
</head>

<body>
  <div class="wrapper">
    <h1>Do LLMs have Personality?</h1>
    <h4>Spring 2025 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
    <h4>Nvidia and Chill</h4>

    <div class="authors-wrapper">
      <div class="author-container">
        <div class="author-image">
          <img src="./files/avatar.png" alt="Vaibhav Jain">
        </div>
        <p>Vaibhav Jain</p>
      </div>
    </div>

    <div class="publication-links">
      <span class="link-block">
        <a href="" class="button is-normal is-rounded is-dark is-outlined" target="_blank">Final Report</a>
      </span>
      <span class="link-block">
        <a href="" class="button is-normal is-rounded is-dark is-outlined" target="_blank">Code</a>
      </span>
      <span class="link-block">
        <a href="" class="button is-normal is-rounded is-dark is-outlined" target="_blank">Model Weights</a>
      </span>
    </div>

    <hr>

    <h2 id="abstract">Abstract</h2>
    <p>
      This project explores how different Large Language Models (LLMs) respond to the same personality questionnaire (the Big Five Inventory Short Form, BFI-2-S) when prompted to answer as if they belong to specific demographic profiles. By comparing model outputs, we observe how demographic context might shape perceived personality traits. The findings demonstrate variability in how each model “interprets” demographic cues, opening the door for future research into model biases, controllable text generation, and personalization.
    </p>

    <hr>
    <h2 id="teaser">Teaser Figure</h2>
    <p>A figure that conveys the main idea behind the project. This figure is from <a href="https://arxiv.org/pdf/2210.07469">StyLEx</a>.</p>
    <p class="sys-img"><img src="./files/teaser.png" alt="Teaser"></p>

    <hr>
    <h2 id="introduction">Introduction / Background / Motivation</h2>
    <p><strong>What did you try to do? What problem did you try to solve?</strong></p>
    <p>
      With the rapid growth of open-source LLMs, researchers are interested in how these models adapt responses based on user or persona prompts. Personality questionnaires like BFI-2-S offer a standardized way to simulate personality attributes.
    </p>
    <p>
      Our goal is to investigate if LLM responses to BFI-2-S vary predictably when asked to adopt different demographic backgrounds.
    </p>

    <hr>
    <h2 id="approach">Methodology</h2>
    <p><strong>1. Demographic Profiles</strong></p>
    <ul>
      <li>18-year-old male, high school student, United States</li>
      <li>25-year-old female, graduate student, Germany</li>
      <li>30-year-old, software engineer, India</li>
      <li>40-year-old male, teacher, Brazil</li>
      <li>22-year-old female, college student, Nigeria</li>
      <li>35-year-old male, office worker, Japan</li>
      <li>50-year-old female, nurse, United Kingdom</li>
      <li>28-year-old nonbinary, freelance artist, Canada</li>
      <li>60-year-old male, retired professor, South Korea</li>
      <li>45-year-old female, entrepreneur, Egypt</li>
    </ul>

    <p><strong>2. BFI-2-S 30-Item Questionnaire</strong></p>
    <p>Each statement rated on a scale of 1–5. “I am someone who...”</p>
    <p><img src="./files/bfi.png" alt="BFI Questionnaire"></p>

    <p><strong>3. Prompt Template</strong></p>
    <p>Example demographic prompt used for all profiles:</p>
    <p><img src="./files/prompt.png" alt="Prompt Example"></p>

    <p><strong>Models Used:</strong></p>
    <ul>
      <li>Qwen/Qwen2.5-3B-Instruct</li>
      <li>Meta-Llama/Llama-3.2-3B-Instruct</li>
      <li>Mistral/Mistral-7B-Instruct</li>
    </ul>

    <hr>
    <h2 id="results">Results</h2>
    <p>Results include both quantitative and qualitative differences in how models responded.</p>

    <table>
      <caption>Table 1. Summary of Errors Across Examples</caption>
      <thead>
        <tr><th>Experiment</th><th>1</th><th>2</th><th>3</th></tr>
      </thead>
      <tbody>
        <tr><td>Sentence</td><td>Example 1</td><td>Example 2</td><td>Example 3</td></tr>
        <tr><td>Errors</td><td>error A, B, C</td><td>error C</td><td>error B</td></tr>
      </tbody>
    </table>

    <div style="text-align: center;">
      <img src="./files/results.png" alt="Results" style="height: 300px;">
    </div>

    <hr>
    <h2 id="conclusion">Conclusion and Future Work</h2>
    <p>
      By administering the BFI-2-S questionnaire to LLaMA, Qwen, and Mistral under 10 demographic prompts, we observe LLMs can generate distinct personality profiles from standardized inputs.
    </p>
    <ul>
      <li><strong>Biases:</strong> Models may embed cultural/age/occupation stereotypes.</li>
      <li><strong>Personalization:</strong> Prompts offer a route to context-aware generation.</li>
      <li><strong>Future Work:</strong> Further evaluation on real human baselines, prompt refinement, and bias mitigation.</li>
    </ul>

  </div>
</body>
</html>
