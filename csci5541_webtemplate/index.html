<!DOCTYPE html>
<html lang="en-US">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project | Spring 2025 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />
  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">
</head>

<body>
  <div class="wrapper">
    <h1 class="title">Do LLMs Have Personality?</h1>
    <h4 class="subtitle">Spring 2025 CSCI 5541 NLP: Class Project – University of Minnesota</h4>
    <h4 class="subtitle">Nvidia and Chill</h4>
    <hr>

    <section id="abstract">
      <h2 class="heading">Abstract</h2>
      <p>
        This project explores how different Large Language Models (LLMs) respond to the same personality questionnaire (the
        Big Five Inventory Short Form, BFI-2-S) when prompted to answer as if they belong to specific demographic profiles.
        By comparing model outputs, we observe how demographic context shapes perceived personality traits. Our findings
        reveal stable, model-specific behavior signatures, highlighting implications for alignment, fairness, and
        personalization.
      </p>
    </section>

    <hr>

    <section id="introduction">
      <h2 class="heading">Introduction & Background</h2>
      <p><strong>Problem:</strong> As open-source LLMs proliferate, understanding their implicit behavioral patterns—such
      as “personality”—becomes crucial for safe, trustworthy deployment.</p>
      <p><strong>Goal:</strong> Administer a standard Big Five psychometric test to multiple LLMs across 540 synthetic
      demographic profiles and quantify emergent trait differences.</p>
    </section>

    <hr>

    <section id="methodology">
      <h2 class="heading">3. Methodology</h2>

      <h3 class="subheading">3.1 Synthetic Profile Generation</h3>
      <p>
        We generated 540 synthetic “users” by crossing four demographic dimensions:
      </p>
      <ul>
        <li><strong>Age (6):</strong> 18–25, 26–35, 36–45, 46–55, 56–65, 66+ years</li>
        <li><strong>Gender (3):</strong> Male, Female, Non-binary</li>
        <li><strong>Location (6):</strong> USA, Germany, India, Brazil, Japan, Australia</li>
        <li><strong>Education (5):</strong> High School, Associate’s, Bachelor’s, Master’s, Doctorate</li>
      </ul>
      <p>
        Example: “I am a 36–45 year-old non-binary with a master’s degree living in India.”
      </p>

      <h3 class="subheading">3.2 Prompt Design</h3>
      <p>
        For each profile and each of the 30 BFI-2-S items, we issued a two-part chat prompt:
      </p>
      <ol>
        <li>
          A <em>system</em> message: <code>"You are a helpful assistant completing a Big Five personality survey."</code>
        </li>
        <li>
          A <em>user</em> message combining:
          <ul>
            <li>The profile sentence</li>
            <li>An instruction header: <code>"Answer with a single number: 1 = strongly disagree … 5 = strongly agree. No extra text."</code></li>
            <li>The BFI-2-S statement, wrapped in backticks</li>
          </ul>
        </li>
      </ol>
      <figure class="image is-3by1">
        <img src="./files/bfi-2.png" alt="Example prompt template">
        <figcaption>Figure 1: Prompt template combining demographic context, instructions, and a BFI-2-S item.</figcaption>
      </figure>

      <h3 class="subheading">3.3 Model Inference</h3>
      <p>
        We evaluated four open-source, instruction-tuned LLMs (Gemma-3-1B-it, Qwen-2.5-3B, LLaMA-3.2-1B, LLaMA-3.2-3B)
        on an NVIDIA A100 GPU using the vLLM framework. Key settings:
      </p>
      <ul>
        <li>Deterministic decoding (temperature = 0.0, max_tokens = 4)</li>
        <li>Batched inference of 16200 prompts/model (540 profiles × 30 items)</li>
        <li>GPU utilization capped at 95 %</li>
      </ul>

      <h3 class="subheading">3.4 Response Parsing & Scoring</h3>
      <p>
        We extracted the first valid digit (1–5) from each response via regex, retried invalid outputs (< 0.2 %),
        then applied the BFI-2-S scoring key:
      </p>
      <ul>
        <li>Reverse-score specified items: <code>rating ← (6 – rating)</code></li>
        <li>Average the six item ratings per trait to get per-profile means</li>
      </ul>

      <h3 class="subheading">3.5 Statistical Summaries</h3>
      <p>
        Using Pandas, we computed each model’s mean and standard deviation for all five traits across 540 profiles.
        These descriptive statistics form the basis of our quantitative comparison.
      </p>
    </section>

    <hr>

    <section id="results">
      <h2 class="heading">4. Results</h2>
      <p>
        We summarize two views: (1) tabular trait means, and (2) comparative plots showing trait differences.
      </p>

      <h3 class="subheading">4.1 Trait Means Table</h3>
      <table class="table is-striped is-fullwidth">
        <thead>
          <tr>
            <th>Model</th>
            <th>Extraversion</th>
            <th>Agreeableness</th>
            <th>Conscientiousness</th>
            <th>Neuroticism</th>
            <th>Openness</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Gemma-3-1B-it</td>
            <td>4.19</td><td>4.60</td><td>4.40</td><td>1.89</td><td>4.20</td>
          </tr>
          <tr>
            <td>Qwen-2.5-3B</td>
            <td>3.75</td><td>4.05</td><td>3.90</td><td>2.60</td><td>4.00</td>
          </tr>
          <tr>
            <td>LLaMA-3.2-1B</td>
            <td>3.45</td><td>3.50</td><td>3.40</td><td>3.35</td><td>3.60</td>
          </tr>
          <tr>
            <td>LLaMA-3.2-3B</td>
            <td>3.42</td><td>3.55</td><td>3.48</td><td>3.30</td><td>3.55</td>
          </tr>
        </tbody>
      </table>

      <h3 class="subheading">4.2 Comparative Plots</h3>
      <p>Bar chart with error bars (mean ± std):</p>
      <figure class="image is-3by2">
        <img src="./files/traits_across_models.png" alt="Bar chart of trait comparison">
        <figcaption>Figure 2: Mean trait scores across models, with standard deviation error bars.</figcaption>
      </figure>
    </section>

    <hr>

    <section id="conclusion">
      <h2 class="heading">5. Conclusion & Future Work</h2>
      <p>
        Across 540 demographic contexts, LLMs exhibit stable, distinguishable personality signatures. Gemma shows high
        agreeableness and low neuroticism; LLaMAs remain neutral; Qwen lies in between.
      </p>
      <ul>
        <li><strong>Bias:</strong> Models may encode demographic stereotypes.</li>
        <li><strong>Personalization:</strong> Demographic prompts can steer tone and style.</li>
        <li><strong>Next steps:</strong> Human baseline comparisons, prompt refinement, and bias mitigation strategies.</li>
      </ul>
    </section>

  </div>
</body>
</html>
